{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01023451",
   "metadata": {},
   "source": [
    "# Data management (individual CSV files $\\to$ joint CSV file)\n",
    "\n",
    "STAGE 2 OF THE DATA PIPELINE\n",
    "\n",
    "Things that happen in this script:\n",
    "1. Combine multiple Atlas project CSVs (output by Stage 1) into a single dataframe\n",
    "2. Double-check to remove any duplicate rows, or rows from certain \"test\" chats\n",
    "3. Perform post-hoc codebook edits (merge \"errorLine\" into \"errorLocation\")\n",
    "4. Extract \"interaction\" (or \"conversation,\" or \"issue\")-level information (e.g., outcome) and store it\n",
    "5. Store \"blinded\" versions of the codes (where information such as \"conversation outcome\" is blinded; this is useful for some analyses later on)\n",
    "6. Construct a dataframe indicating which annotators annotated which documents (FIXME programmatically check this against the one in Google Drive)\n",
    "\n",
    "Clean up quote endpoints and extract metadata from multiple Atlas projects, and combine them into a single file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db29a0",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = True\n",
    "\n",
    "input_versions = [14, 15, 16]\n",
    "output_version = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd42402",
   "metadata": {},
   "source": [
    "## Baseline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622df192",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\"../output/v{}/human-readable-annotations.csv\".format(input_version) for input_version in input_versions]\n",
    "\n",
    "output_parent_dir = \"../output/clean\"\n",
    "output_child_dir = \"v{}\".format(output_version)\n",
    "output_dir = os.path.join(output_parent_dir, output_child_dir)\n",
    "\n",
    "output_file = \"annotations_data.csv\"\n",
    "sample_output_file = \"small_\" + output_file\n",
    "output_coders = \"coders_per_document.csv\"\n",
    "\n",
    "# Long-form dataframe of annotations from document-annotator pairs without well-defined\n",
    "# conversation sentinels (excluded from all subsequent dataframes because I can't infer\n",
    "# conversation IDs, types, and outcomes for any of them)\n",
    "excluded_rows_file = \"excluded_annotations.csv\"\n",
    "\n",
    "# List of the chats that were excluded (in the file above)\n",
    "anomalous_chats_file = \"anomalous_chats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    try:\n",
    "        os.mkdir(output_parent_dir)\n",
    "    except FileExistsError:\n",
    "        print(\"High-level output directory already exists; no action taken.\")\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except FileExistsError:\n",
    "        print(\"WARNING: low-level output directory already exists. You might want to increment your version number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b76f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util for displaying dataframes\n",
    "# the defaults are actually 60 & 20, but that gets annoying\n",
    "def show(da, rows = 20, cols = 20):\n",
    "    pd.set_option(\"display.max_rows\", rows)\n",
    "    pd.set_option(\"display.max_columns\", cols)\n",
    "    display(da)\n",
    "    pd.reset_option(\"max_rows\")\n",
    "    pd.reset_option(\"max_columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb6c88",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc31635",
   "metadata": {},
   "outputs": [],
   "source": [
    "das = [pd.read_csv(input_file, index_col = 0) for input_file in input_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545d833",
   "metadata": {},
   "source": [
    "Fix the columns and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'quote.text' : 'string', \n",
    "          'annotation.code' : 'string', \n",
    "          'annotation.creatingUser' : 'string',\n",
    "          'annotation.creationDateTime' : 'string', \n",
    "          'quote.startPosition' : np.int64, \n",
    "          'quote.endPosition' : np.int64, \n",
    "          'quote.creatingUser' : 'string', \n",
    "          'quote.creationDateTime' : 'string', \n",
    "          'quote.modifyingUser' : 'string', \n",
    "          'quote.modifiedDateTime' : 'string', \n",
    "          'document.name' : 'string', \n",
    "          'document.creatingUser' : 'string', \n",
    "          'document.creationDateTime' : 'string', \n",
    "          'document.modifyingUser' : 'string', \n",
    "          'document.modifiedDateTime' : 'string',\n",
    "          'document.plainTextPath' : 'string', \n",
    "          'document.richTextPath' : 'string', \n",
    "          'annotation.guid' : 'string', \n",
    "          'annotation.codeRef.guid' : 'string', \n",
    "          'quote.guid' : 'string', \n",
    "          'document.guid' : 'string', \n",
    "          'quote.paragraphStartPosition' : np.int64, \n",
    "          'quote.paragraphEndPosition' : np.int64,\n",
    "          'quote.paragraphText' : 'string', \n",
    "          'quote.speaker' : np.int64, \n",
    "          'quote.speakerIsLearner' : bool}\n",
    "defaults = {np.int64 : \"-1\", \"string\" : \"N/A\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for da in das:\n",
    "    for col in dtypes.keys():\n",
    "        if not col in da.columns:\n",
    "            da[col] = defaults[dtypes[col]] # to deal with missing info from Version 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46345551",
   "metadata": {},
   "source": [
    "Now combine things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = pd.concat(das).drop_duplicates().reset_index(drop=True)\n",
    "#da = pd.read_csv(input_files[0], index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54862851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(da, rows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b77bb",
   "metadata": {},
   "source": [
    "## Remove test rows\n",
    "This should now be redundant to steps completed in `data-management_qdpx-to-csv.ipynb`, so just check that the printed output agrees with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebf840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to keep these\n",
    "print(da[da[\"document.name\"].str.endswith(\".txt\")].shape)\n",
    "\n",
    "# but not these\n",
    "print(da[~da[\"document.name\"].str.endswith(\".txt\")].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef082e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1 = da[da[\"document.name\"].str.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1[\"annotation.creatingUser\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da75c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(da1.iloc[10][\"quote.endPosition\"] - da1.iloc[10][\"quote.startPosition\"])\n",
    "print(len(da1.iloc[10][\"quote.text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1.iloc[10][\"quote.text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f1153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a566070",
   "metadata": {},
   "source": [
    "## Post-hoc codebook edits\n",
    "\n",
    "Merge: \n",
    "1. \"errorLine\" and \"errorLocation\"\n",
    "2. \"aggression\" and \"frustration\"\n",
    "3. \"phrasedAsQuestion\" and \"phrasedAsUnsure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71488a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1[\"annotation.original_code\"] = da1[\"annotation.code\"]\n",
    "da1[\"annotation.code\"] = da1[\"annotation.code\"].replace(\n",
    "    \"General message attributes > contentDomain > errorLine\", \n",
    "    \"General message attributes > contentDomain > errorLocation\")\n",
    "da1[\"annotation.code\"] = da1[\"annotation.code\"].replace(\n",
    "    \"Attitude, tone, or mood > expressNegativity > aggression\", \n",
    "    \"Attitude, tone, or mood > expressNegativity > frustration\")\n",
    "da1[\"annotation.code\"] = da1[\"annotation.code\"].replace(\n",
    "    \"Explanations and help > confidenceLevel > phrasedAsQuestion\", \n",
    "    \"Explanations and help > confidenceLevel > phrasedAsUnsure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "annls = np.sort(da1[\"annotation.code\"].unique())\n",
    "len(annls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38498e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annmp = {x : i for i, x in enumerate(annls)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d72408",
   "metadata": {},
   "source": [
    "## Remove any duplicates and sort the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151430d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "# even though it would be counter-intuitive later on, we need to sort by annotator here\n",
    "# in order for the conversation labelling to run smoothly\n",
    "important_cols = [\"document.name\", \"annotation.creatingUser\", \n",
    "                  \"quote.startPosition\", \"quote.endPosition\", \n",
    "                  \"annotation.code\"]\n",
    "print(da1.shape)\n",
    "print(da1[important_cols].drop_duplicates().shape)\n",
    "print(\"Duplicates present? {}.\".format(\"No\" if len(da1) == len(da1[important_cols].drop_duplicates()) else \"Yes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bf27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of duplicates (this also sorts the dataframe yay)\n",
    "da2 = da1.reset_index().rename(columns={\"index\" : \"da1.idx\"}).groupby(by=important_cols).agg(\"first\")\n",
    "\n",
    "# reindex and revert the column ordering\n",
    "da2 = da2.reset_index()[da1.columns.insert(0, \"da1.idx\")]\n",
    "da2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1379dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(da2.sort_values(by=important_cols) == da2).all().all() # check that it's sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f138d",
   "metadata": {},
   "source": [
    "## Label the \"interactions/conversations\" (requests) and their outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390361c",
   "metadata": {},
   "source": [
    "### First check for issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the indices for request and resolveRequest annotations in a chat, \n",
    "# validate that they bookend valid ranges of annotations\n",
    "def validate_interactions(reqls, resls):\n",
    "    # edge case\n",
    "    if len(reqls) == 0 or len(resls) == 0:\n",
    "        return False\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    # for each interaction\n",
    "    while i < len(reqls) and j < len(resls):\n",
    "        # check that the starting point is valid\n",
    "        if reqls[i] > resls[j]:\n",
    "            return False\n",
    "        \n",
    "        # read all requests in this interaction\n",
    "        while i < len(reqls) and j < len(resls) and reqls[i] < resls[j]:\n",
    "            i += 1\n",
    "        \n",
    "        # move to the next interaction\n",
    "        j += 1\n",
    "        \n",
    "    return i == len(reqls) and j == len(resls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "validate_interactions([10, 18, 44], [31, 110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing data (documents with missing interaction info)\n",
    "invalid_chats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in da2[\"document.name\"].unique():\n",
    "    users = da2[da2[\"document.name\"] == doc][\"annotation.creatingUser\"].unique()\n",
    "    for user in users:\n",
    "        da3 = da2[(da2[\"document.name\"] == doc) & (da2[\"annotation.creatingUser\"] == user)]\n",
    "        da3 = da3.reset_index().rename(columns={\"index\" : \"da1.idx\"})\n",
    "    \n",
    "        requestls = da3[da3[\"annotation.code\"].str.startswith(\n",
    "            \"Big picture of an interaction > request\")].index.values.tolist()\n",
    "        \n",
    "        resolvedls = da3[da3[\"annotation.code\"].str.startswith(\n",
    "            \"Big picture of an interaction > resolveRequest\")].index.values.tolist()\n",
    "        \n",
    "        if not validate_interactions(requestls, resolvedls):\n",
    "            print(\"[Failed validation] Document {} annotated by {}\".format(doc, user))\n",
    "            invalid_chats[(doc, user)] = (requestls.copy(), resolvedls.copy())\n",
    "        \n",
    "        #problems = []\n",
    "        #if not validate_interactions(requestls, resolvedls):\n",
    "        #    problems.append(\"Failed validation\")\n",
    "        #    printed_something = True\n",
    "        \n",
    "        #annotated = True\n",
    "        #try:\n",
    "        #    annotated = document_metadata.loc[doc, user.split(\" \")[0]]\n",
    "        #except(KeyError):\n",
    "        #    annotated = True # M isn't in the spreadsheet\n",
    "        \n",
    "        #if len(problems) > 0:\n",
    "        #    problems = \", \".join(problems)\n",
    "        #    print(\"[{}]{} Document {} annotated by {}\".format(problems, \" \" * (37 - len(problems)), doc, user))\n",
    "        #    invalid_chats[(doc, user)] = (requestls.copy(), resolvedls.copy(), annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (doc, user), (req, res) in invalid_chats.items():\n",
    "    print( # (\"ok\" if not ann else \"  \"), \" \",\n",
    "          doc, \"   \", \n",
    "          user, \" \" * (40 - len(doc) - len(user)), \n",
    "          req, \"vs.\", res #, \" \" * (50 - len(req) - len(res)), \n",
    "          )\n",
    "    #print(req)\n",
    "    #print(res)\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_da = {(d, u) : {\"Request indices\" : req, \n",
    "                        \"Resolved indices\" : res, \n",
    "                        # \"Was annotated\" : ann\n",
    "                       } \n",
    "              for (d, u), (req, res) in invalid_chats.items()}\n",
    "invalid_chats_da = pd.DataFrame.from_dict(data = dict_to_da, orient = \"index\", dtype=str)\n",
    "invalid_chats_da = invalid_chats_da.reset_index()\n",
    "invalid_chats_da = invalid_chats_da.rename(columns={\"level_0\" : \"Document\", \n",
    "                                                    \"level_1\" : \"Annotator\"})\n",
    "invalid_chats_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_start = invalid_chats_da[\"Request indices\"] == \"[]\"\n",
    "invalid_chats_da[\"Problem\"] = np.where(missing_start, \"No start labels\", \"\")\n",
    "\n",
    "missing_end = invalid_chats_da[\"Resolved indices\"] == \"[]\"\n",
    "invalid_chats_da[\"Problem\"] = np.where(missing_end, \"No end labels\", invalid_chats_da[\"Problem\"])\n",
    "\n",
    "missing_both = (invalid_chats_da[\"Request indices\"] == \"[]\") & (invalid_chats_da[\"Resolved indices\"] == \"[]\")\n",
    "invalid_chats_da[\"Problem\"] = np.where(missing_both, \"No start or end labels\", invalid_chats_da[\"Problem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26c294",
   "metadata": {},
   "source": [
    "Now do the one-offs, if needed (it wasn't needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_chats_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef2e06",
   "metadata": {},
   "source": [
    "Code for testing the chats extracted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5796114",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, user = \"451n3l6h9l.txt\", \"A\"\n",
    "invalid_chats[(doc, user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63acaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = da2[(da2[\"document.name\"] == doc) & (da2[\"annotation.creatingUser\"] == user)]\n",
    "tmp = tmp.reset_index().rename(columns={\"index\" : \"da1.idx\"})\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c834cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "ii = np.concatenate(invalid_chats[(doc, user)])\n",
    "ii.sort()\n",
    "tmp.iloc[ii][[\"da1.idx\", \n",
    "              \"quote.text\", \n",
    "              \"annotation.code\", \n",
    "              \"annotation.creatingUser\", \n",
    "              \"annotation.creationDateTime\", \n",
    "              \"quote.startPosition\", \n",
    "              \"quote.endPosition\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ad263",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310da94d",
   "metadata": {},
   "source": [
    "Remove the chats that failed validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076f423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invalid_rows = pd.Series(data=zip(da2[\"document.name\"], da2[\"annotation.creatingUser\"]), \n",
    "                         index=da2.index).apply(lambda x : x in invalid_chats.keys())\n",
    "da3 = da2[~invalid_rows].reset_index().rename(columns={\"index\" : \"da2.idx\"})\n",
    "da3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    invalid_chats_da.to_csv(os.path.join(output_dir, anomalous_chats_file))\n",
    "    da2[invalid_rows].to_csv(os.path.join(output_dir, excluded_rows_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cdebf",
   "metadata": {},
   "source": [
    "### Now actually label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96406b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the indices for request and resolveRequest annotations in a chat, \n",
    "# output the number of interactions and their bounds (as indices)\n",
    "# this should only be run on inputs that pass validate_interactions()\n",
    "def extract_interactions(reqls, resls):\n",
    "    interactions = []\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    # for each interaction\n",
    "    while i < len(reqls) and j < len(resls):\n",
    "        k = i\n",
    "        \n",
    "        # read all requests in this interaction\n",
    "        while i < len(reqls) and j < len(resls) and reqls[i] < resls[j]:\n",
    "            i += 1\n",
    "        \n",
    "        interactions.append((reqls[k:i], resls[j])) # inclusive, exclusive\n",
    "        \n",
    "        # move to the next interaction\n",
    "        j += 1\n",
    "        \n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "print(extract_interactions([1, 2, 5], [3, 6]))\n",
    "print(extract_interactions([10, 20, 50], [30, 60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_idx = pd.Series(data=-1, index=da3.index)         # index for interactions in a document\n",
    "interaction_len = pd.Series(data=0, index=da3.index)          # length of this annotation's interaction\n",
    "strict_interaction_len = pd.Series(data=0, index=da3.index)   # length of this annotation's interaction\n",
    "requests = pd.Series(data=\"N/A\", index=da3.index)             # comma-separated list of requests for \n",
    "                                                              # this interaction (leaf annotation only)\n",
    "outcomes = pd.Series(data=\"N/A\", index=da3.index)             # \"S\" for successes and \"F\" for failures\n",
    "well_defined = pd.Series(data=False, index=da3.index)         # True iff the interaction number is unambiguous\n",
    "\n",
    "assert(len(outcomes) == len(da3))\n",
    "interaction_idx.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc1e60",
   "metadata": {},
   "source": [
    "The way I've implemented this currently, any Interaction has exactly one combination of Requests. This keeps the code/analysis simpler, but it also means that for interactions where a new request is made partway through, all of the previous annotations in the interaction will still identify with that request type even though it hasn't been made yet.\n",
    "\n",
    "This might change depending on the requirements of later analyses.\n",
    "\n",
    "Update: In all recent versions of the input data, each Interaction should have *exactly* one Request, so this shouldn't be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert((da3.sort_values(by=important_cols) == da3).all().all()) # check that it's still sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1256ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in da3[\"document.name\"].unique():\n",
    "    users = da3[da3[\"document.name\"] == doc][\"annotation.creatingUser\"].unique()\n",
    "    for user in users:\n",
    "        da4 = da3[(da3[\"document.name\"] == doc) & (da3[\"annotation.creatingUser\"] == user)]\n",
    "        da4 = da4.reset_index().rename(columns={\"index\" : \"da3.idx\"})\n",
    "    \n",
    "        requestls = da4[da4[\"annotation.code\"].str.startswith(\n",
    "            \"Big picture of an interaction > request\")].index.values.tolist()\n",
    "        \n",
    "        resolvedls = da4[da4[\"annotation.code\"].str.startswith(\n",
    "            \"Big picture of an interaction > resolveRequest\")].index.values.tolist()\n",
    "        \n",
    "        # write interaction info into the Serieses defined above\n",
    "        interactionls = extract_interactions(requestls, resolvedls)\n",
    "        persistent_weak_start = 0 # start index that lumps preceding ambiguous annotations into each interaction\n",
    "        for n, (ireqls, ires) in enumerate(interactionls): # for each interaction\n",
    "            # get the request type(s)\n",
    "            reqls = da4.loc[ireqls, \"annotation.code\"].str.split(\" > \").str[-1].unique() # list of req\n",
    "            reqls.sort() # alphabetize\n",
    "            reqstr = \", \".join(reqls)\n",
    "            \n",
    "            # get the outcome\n",
    "            res = da4.loc[ires, \"annotation.code\"].split(\" > \")[-1][0].capitalize() # \"S\" or \"F\"\n",
    "            assert(res in {\"S\", \"F\"}) # double-check\n",
    "\n",
    "            # get the earliest start index and latest end index of the interaction (wrt da4)\n",
    "            start4 = ireqls[0]\n",
    "            while (start4 > 0 and \n",
    "                   da4.loc[start4-1, \"quote.startPosition\"] >= da4.loc[ireqls[0], \"quote.startPosition\"]):\n",
    "                start4 -= 1\n",
    "\n",
    "            stop4 = ires # inclusive\n",
    "            while (stop4+1 < len(da4) and # da4 has consecutive indexing \n",
    "                   da4.loc[stop4+1, \"quote.endPosition\"] <= da4.loc[ires, \"quote.endPosition\"]):\n",
    "                stop4 += 1\n",
    "\n",
    "            # translate da4 indices into da3 indices (the ordering property should be preserved)\n",
    "            start = da4.loc[start4, \"da3.idx\"]\n",
    "            stop = da4.loc[stop4, \"da3.idx\"] + 1 # exclusive (da3 also has consecutive indexing)\n",
    "            \n",
    "            weak_start = da4.loc[persistent_weak_start, \"da3.idx\"]\n",
    "            weak_stop = stop if n < len(interactionls) - 1 else da4.loc[len(da4)-1, \"da3.idx\"] + 1 # exclusive\n",
    "            \n",
    "            # write the information to the output Serieses\n",
    "            interaction_idx.iloc[weak_start:weak_stop] = n\n",
    "            interaction_len.iloc[weak_start:weak_stop] = weak_stop - weak_start\n",
    "            requests.iloc[weak_start:weak_stop] = reqstr\n",
    "            outcomes.iloc[weak_start:weak_stop] = res\n",
    "            \n",
    "            well_defined[start:stop] = True\n",
    "            strict_interaction_len.iloc[start:stop] = stop - start\n",
    "            \n",
    "            # update the weak interaction boundary index\n",
    "            persistent_weak_start = stop4+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was the old way of checking this; it returned 31725 on input versions [14, 15, 16] and they match, yay!\n",
    "#well_defined = (interaction_idx != -1) \n",
    "well_defined.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c47d1a",
   "metadata": {},
   "source": [
    "Not all rows belong to a well-defined interaction (some are in-between sentinels).\n",
    "\n",
    "We clean up by grouping orphaned rows with the next interaction (not the previous one, since they can't have causally impacted the result) where possible. However, orphaned rows that are actually at the end of the chat document will be grouped with the previous interaction out of necessity.\n",
    "\n",
    "We also leave an `interaction.strict` column providing context for these rows.\n",
    "\n",
    "Time permitting, I'll come back here and leave an `interaction.alt_number` column too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d241a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# da3 = da3[well_defined].reset_index(drop=True)\n",
    "# interaction_idx = interaction_idx[well_defined].reset_index(drop=True)\n",
    "# requests = requests[well_defined].reset_index(drop=True)\n",
    "# outcomes = outcomes[well_defined].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_idx.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c4705",
   "metadata": {},
   "source": [
    "Write the above results into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ae738",
   "metadata": {},
   "outputs": [],
   "source": [
    "da3[\"interaction.number\"] = interaction_idx\n",
    "da3[\"interaction.len\"] = interaction_len\n",
    "da3[\"interaction.requests\"] = requests\n",
    "da3[\"interaction.outcome\"] = outcomes\n",
    "da3[\"interaction.strict\"] = well_defined\n",
    "\n",
    "da3[\"interaction.strict_len\"] = strict_interaction_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf474ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_idx.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4469e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13348134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "requests.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91cf89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77d387e",
   "metadata": {},
   "source": [
    "## Make columns for blinded code labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0fe92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# codes column, but with outcomes masked (requests are unmasked)\n",
    "da3[\"annotation.code.noOutcome\"] = np.where(\n",
    "    da3[\"annotation.code\"].str.startswith(\"Big picture of an interaction > resolveRequest\"), \n",
    "    \"Big picture of an interaction > resolveRequest\", \n",
    "    da3[\"annotation.code\"])\n",
    "\n",
    "# codes column, but with both requests and outcomes masked\n",
    "da3[\"annotation.code.noRequestOutcome\"] = np.where(\n",
    "    da3[\"annotation.code\"].str.startswith(\"Big picture of an interaction > request\"), \n",
    "    \"Big picture of an interaction > request\", \n",
    "    da3[\"annotation.code.noOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6791e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69df3912",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "da3[\"document.creationDateTime\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    da3.to_csv(os.path.join(output_dir, output_file))\n",
    "    da3.head(20).to_csv(os.path.join(output_dir, sample_output_file)) # for easy visualization on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa0d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0144f3",
   "metadata": {},
   "source": [
    "## Figure out who coded which documents\n",
    "\n",
    "FIXME check these against the spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = np.array([[coder, (lambda x, coder=coder : (x == coder).any())] # new syntax yay\n",
    "                       for coder in da3[\"annotation.creatingUser\"].unique()])\n",
    "\n",
    "for i in range(indicators.shape[0]): # loop over the functions\n",
    "    print(\"(function) {}:\".format(indicators[i, 0])) # name associated with the function we're testing\n",
    "    for j in range(indicators.shape[0]): # loop over the inputs\n",
    "        print(\"== (input) {}? \\t\\t\\t{}\".format(indicators[j, 0], indicators[i, 1](pd.Series([indicators[j, 0]])))) # test input j against function i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "n = 3000\n",
    "step = 10\n",
    "minida1 = da3.iloc[::step, :].head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbe9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display(da1.head(1000).groupby(by=\"document.name\", as_index=False).agg({\"annotation.creatingUser\" : lambda x : (x == \"A\").any()}))\n",
    "\n",
    "n = 300\n",
    "minida2 = minida1.head(n)\n",
    "\n",
    "for k in range(indicators.shape[0]):\n",
    "    print(\"{}:\".format(indicators[k, 0]))\n",
    "    show(minida2.groupby(by=\"document.name\", as_index=False).agg({\"annotation.creatingUser\" : indicators[k, 1]}))\n",
    "\n",
    "for dname in minida2[\"document.name\"].unique():\n",
    "    tmpda = minida2[minida2[\"document.name\"] == dname]\n",
    "    print(\"Document: {}\".format(dname))\n",
    "    print(tmpda[\"annotation.creatingUser\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f279755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documentinfo = list(filter(lambda col : col.startswith(\"document.\"), da3.columns))\n",
    "dda = da3.groupby(by=\"document.name\", \n",
    "                  as_index=False).agg({**{col : [lambda x : x.iloc[0]] for col in documentinfo}, \n",
    "                                       **{\"annotation.creatingUser\" : indicators[:, 1]}})\n",
    "dda.columns = np.concatenate((dda.columns.get_level_values(level=0)[:-len(indicators[:, 0])], # other metadata fields\n",
    "                              indicators[:, 0])) # people names\n",
    "dda.head(3) # don't worry, not all of them are True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3094315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prints the number of documents coded by each combination of people (have fun with inclusion-exclusion!)\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))\n",
    "\n",
    "coders = indicators[:, 0]\n",
    "for subset in powerset(coders):\n",
    "    print(\", \".join(list(subset)), \" : \", dda[dda[list(subset)].all(axis=1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a4c29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# documents seen by all 4 coders\n",
    "subset = list(powerset(coders))[-1]\n",
    "print(\"\\n\".join(list(dda[dda[list(subset)].all(axis=1)][\"document.name\"]))) # note this doesn't re-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049785e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dda[dda[list(subset)].all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prints how many annotations each person made on each document\n",
    "for dname in da3[\"document.name\"].unique():\n",
    "    tmpda = da3[da3[\"document.name\"] == dname]\n",
    "    print(\"Document: {}\".format(dname))\n",
    "    print(tmpda[\"annotation.creatingUser\"].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958241c",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    dda.to_csv(os.path.join(output_dir, output_coders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b946f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
