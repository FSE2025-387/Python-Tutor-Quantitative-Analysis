{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f42fa8d",
   "metadata": {},
   "source": [
    "# Dataframe Preparation for Generalized Linear Regression Analysis of Annotated Chats\n",
    "\n",
    "Also low-rank SVD stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8158a7",
   "metadata": {},
   "source": [
    "## README\n",
    "\n",
    "Section A of this document constructs long-form versions of the data under various codebooks:\n",
    "1. **Codebook 1**: The size-48 codebook consisting of all leaf nodes, with no programmatic incorporation of the multi-attribute structure for Helping and Questioning quotations that we enforced manually during the annotation phase.\n",
    "2. **Codebook 2**: The size-897 (or 491 without \"unknown\" attribute options) codebook in which every possible combination of attributes assigned to a Helping or Questioning instance is treated as an individual code (and attributes cannot occur outside of this structure).\n",
    "3. **Codebook 3**: The size-197 codebook constructed by dropping confidence and specificity information from Codebook 2.\n",
    "4. **Codebook 4**: The size-92 codebook derived from Codebook 3 via the following steps: \\\n",
    "   a) Merge \"guiding\" questions into \"guide interactively\" help \\\n",
    "   b) Merge positive and negative confirmation into \"confirmation\" \\\n",
    "   c) Group contentDomains per the hierarchy (Figure 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c892ab",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ffbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from scipy.stats.distributions import norm\n",
    "import statistics\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_version = 7\n",
    "input_file = \"../data-management/output/clean/v{}/annotations_data.csv\".format(input_version)\n",
    "document_metadata_file = \"../data-management/data/annotation-timeline.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = True\n",
    "output_version = 5\n",
    "\n",
    "outputdir = \"derived-dataframes/regression-data-v{}\".format(output_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf67c6",
   "metadata": {},
   "source": [
    "Names of files that dataframes are output to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-form dataframe of annotations with Outcomes Codebook\n",
    "# Output from Section A.1\n",
    "codebook1_annotations_output = \"codebook1_longform.csv\"\n",
    "codebook1_codes_output = \"codebook1_codes.csv\" # not using this atm because it's easy to get\n",
    "\n",
    "# Long-form dataframe of annotations with Questioning/Helping Codebook\n",
    "# Output from Section A.2\n",
    "codebook2_annotations_output = \"codebook2_longform.csv\"\n",
    "codebook2_codes_output = \"codebook2_codes.csv\"\n",
    "\n",
    "# Long-form dataframe of annotations with Questioning/Helping minus details Codebook\n",
    "# Output from Section A.3\n",
    "codebook3_annotations_output = \"codebook3_longform.csv\"\n",
    "codebook3_codes_output = \"codebook3_codes.csv\"\n",
    "\n",
    "# Long-form dataframe of annotations with Questioning/Helping grouping attributes Codebook\n",
    "# Output from Section A.4\n",
    "codebook4_annotations_output = \"codebook4_longform.csv\"\n",
    "codebook4_codes_output = \"codebook4_codes.csv\"\n",
    "\n",
    "# Pooled 1-gram counts dataframe, where each row is a code-outcome with its corresponding\n",
    "# number of observations\n",
    "# Output from Section B.1 (the actual outputs are prefixed with the codebook version)\n",
    "code_counts_output = \"code-outcome_counts.csv\"\n",
    "\n",
    "# 1-gram counts dataframe, where each row is a conversation-annotator-code-speaker with\n",
    "# its corresponding outcome and number of observations\n",
    "# Output from Section B.2 (the actual outputs are prefixed with the codebook version)\n",
    "conversation_1gram_counts_output = \"conv-annotator-code-speaker_outcome-counts.csv.gz\"\n",
    "\n",
    "# 1-gram counts dataframe, where each row is a conversation-annotator and each column\n",
    "# is a code-speaker (elements are counts)\n",
    "# Output from Section B.3 (the actual outputs are prefixed with the codebook version)\n",
    "countsmtx_output = \"conv-annotator_code-speaker_counts.csv\"\n",
    "\n",
    "# 2-gram counts dataframe, where each row is a conversation-annotator-code1-speaker1-code2-speaker2\n",
    "# with its corresponding outcome and number of observations\n",
    "# Output from Section C.1\n",
    "conversation_2gram_counts_output = \"conv-annotator-code2-speaker2_outcome-counts.csv.gz\"\n",
    "\n",
    "# the above dataframe is massive (>3 million rows) and doesn't display in github, so we'll also make a 20-line preview\n",
    "# Output from Section C.1\n",
    "small_conversation_2gram_counts_output = \"small_\" + conversation_2gram_counts_output[:-3]\n",
    "\n",
    "if output:\n",
    "    try:\n",
    "        os.mkdir(outputdir)\n",
    "    except FileExistsError:\n",
    "        print(\"Output directory already exists; no action taken.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3580d2c",
   "metadata": {},
   "source": [
    "### Utility to change the pandas dataframe display settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option(\"display.max_colwidth\", None)\n",
    "#pd.reset_option(\"display.max_colwidth\")\n",
    "\n",
    "# util for displaying dataframes\n",
    "# the defaults are actually 60 & 20, but that gets annoying\n",
    "def show(da, rows = 20, cols = 20, width = None):\n",
    "    pd.set_option(\"display.max_rows\", rows)\n",
    "    pd.set_option(\"display.max_columns\", cols)\n",
    "    pd.set_option(\"display.max_colwidth\", width)\n",
    "    display(da)\n",
    "    pd.reset_option(\"max_rows\")\n",
    "    pd.reset_option(\"max_columns\")\n",
    "    pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56155c",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "First read in the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9b520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da0 = pd.read_csv(input_file, index_col=0, keep_default_na=False)\n",
    "da0 = da0.drop([\"da1.idx\", \"da2.idx\"], axis=1)\n",
    "da0.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a9d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da0[\"document.creationDateTime\"].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94251c58",
   "metadata": {},
   "source": [
    "## A. Long-form dataframe manipulation\n",
    "\n",
    "Produces the following dataframes:\n",
    "1. Copy of the input dataframe \\\n",
    "   Codebook size = 48\n",
    "\n",
    "2. Join the co-located Questioning (question, contentDomain, & specificity) and Helping (communicationMechanism, contentDomain, certainty, & specificity) annotations into a single \"Questioning\" or \"Helping\" annotation (still containing all the auxiliary information) \\\n",
    "   Codebook size = 897\n",
    "\n",
    "3. Join the co-located Questioning and Helping annotations like above, but drop less-interesting auxiliary information \\\n",
    "   Codebook size = ~197\n",
    "\n",
    "4. Further coarsening based on the hierarchy \\\n",
    "   Codebook size = idk because I haven't really figured out what this would be yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58176e2d",
   "metadata": {},
   "source": [
    "### 0. Define utilities for column renaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6e5f6",
   "metadata": {},
   "source": [
    "The variable naming here is not ideal for use in R and regression analysis (e.g. column names are too long, \"interaction\" means two things, etc.)\n",
    "\n",
    "We will rename before outputting, but leave the names as-is in this script for legacy reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the renaming maps\n",
    "colname_map = {\"annotation.code\" : \"code\", \n",
    "               \"annotation.creatingUser\" : \"annotator\", \n",
    "               \"document.name\" : \"document\", \n",
    "               \"quote.speaker\" : \"speaker\", \n",
    "               \"quote.speakerIsLearner\" : \"speakerIsLearner\", \n",
    "               \"annotation.code.noOutcome\" : \"code.noOutcome\", \n",
    "               \"annotation.code.noRequestOutcome\" : \"code.noRequestOutcome\"}\n",
    "\n",
    "colterm_map = {\".\" : \"_\", \"interaction\" : \"conversation\"}\n",
    "\n",
    "def replace_colterms(colname, colterm_map = colterm_map):\n",
    "    for old, new in colterm_map.items():\n",
    "        colname = colname.replace(old, new)\n",
    "    return colname\n",
    "\n",
    "def output_longform(da, dirname, fname, colname_map = colname_map, colterm_map = colterm_map):\n",
    "    # real one\n",
    "    da.rename(columns={old : new for old, new in colname_map.items() if old in da.columns}\n",
    "          ).rename(columns={col : replace_colterms(col) for col in da.columns}\n",
    "                  ).to_csv(os.path.join(dirname, fname), index=False)\n",
    "    # mini one for previewing\n",
    "    da.rename(columns={old : new for old, new in colname_map.items() if old in da.columns}\n",
    "          ).rename(columns={col : replace_colterms(col) for col in da.columns}\n",
    "                  ).head(30).to_csv(os.path.join(dirname, \"small_\" + fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13be15",
   "metadata": {},
   "source": [
    "### 1. Construct the first codebook & dataframe (`da1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a6d6f",
   "metadata": {},
   "source": [
    "The main thing we need to do here is \"take votes\" on request type and outcome for each interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the end sentinels because there's exactly one per conversation-annotator (verified in csv cleaning script)\n",
    "votesda = da0[da0[\"annotation.code\"].str.startswith(\"Big picture of an interaction > resolveRequest\")]\n",
    "\n",
    "# take the vote (mode) across annotators for each conversation\n",
    "reqcol = votesda.groupby(by=[\"document.name\", \"interaction.number\"]).aggregate({\"interaction.requests\" : statistics.mode})\n",
    "\n",
    "# rename for later\n",
    "reqcol = reqcol.rename(columns={\"interaction.requests\" : \"voted.interaction.requests\"})\n",
    "reqcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the vote (mode) across annotators for each conversation\n",
    "outcol = votesda.groupby(by=[\"document.name\", \"interaction.number\"]).aggregate({\"interaction.outcome\" : statistics.mode})\n",
    "\n",
    "# rename for later\n",
    "outcol = outcol.rename(columns={\"interaction.outcome\" : \"voted.interaction.outcome\"})\n",
    "outcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6040e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da1 = pd.merge(da0, reqcol, how=\"left\", on=[\"document.name\", \"interaction.number\"])\n",
    "da1 = pd.merge(da1, outcol, how=\"left\", on=[\"document.name\", \"interaction.number\"])\n",
    "da1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed77ffa",
   "metadata": {},
   "source": [
    "#### Generate the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1_codes = np.sort(da1[\"annotation.code\"].unique()).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192de83",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf39390",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    #da1.rename(columns={old : new for old, new in colname_map.items() if old in da1.columns}\n",
    "    #          ).rename(columns={col : replace_colterms(col) for col in da1.columns}\n",
    "    #                  ).to_csv(os.path.join(outputdir, annotations_output), index=False)\n",
    "    output_longform(da1, outputdir, codebook1_annotations_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dfb82b",
   "metadata": {},
   "source": [
    "### 2. Construct the second codebook & dataframe (`da2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd19c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that these output the same thing, so quote GUIDs are unique the way I want them\n",
    "#da1.groupby(by=\"quote.guid\").count()[\"da2.idx\"].value_counts()\n",
    "da1.groupby(by=[\"document.name\", \"annotation.creatingUser\", \"quote.guid\"]).count()[\"quote.text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the result dataframe\n",
    "da2 = da1.copy()\n",
    "#da2[\"annotation.mainCodebook.code\"] = da2[\"annotation.code\"] # rows will change so this is useless\n",
    "\n",
    "# drop columns that won't be well-defined anymore\n",
    "da2 = da2.drop([\"annotation.creationDateTime\", \"annotation.guid\", \n",
    "                \"annotation.codeRef.guid\", \"annotation.original_code\"], \n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc86121",
   "metadata": {},
   "source": [
    "#### a. Define utility functions for dealing with screwed up annotation clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function: given multiple contentDomains, choose the highest-priority\n",
    "# and breaking ties in favor of the less-frequent code\n",
    "contentDomainsByCount = da2.loc[da1[\"annotation.code\"].str.startswith(\"General message attributes > contentDomain\"), \n",
    "                                \"annotation.code\"].str.split(\" > \").str[-1].value_counts()\n",
    "contentDomainsByCount = contentDomainsByCount.sort_values(ascending=True).index.to_list()\n",
    "contentDomainsByCount = {i : k for k, i in enumerate(contentDomainsByCount)}\n",
    "#print(contentDomainsByCount)\n",
    "\n",
    "contentDomainPriorities = {\"bug\" : 0, \n",
    "                           \"codeSpecifications\" : 0, \n",
    "                           \"codingConcept\" : 0, \n",
    "                           \"learningResources\" : 0, \n",
    "                           \"developmentStrategy\" : 0, \n",
    "                           \"testCases\" : 0, \n",
    "                           \"codingExperience\" : 0, \n",
    "                           \"errorMsg\" : 1, \n",
    "                           \"errorLine\" : 2, \n",
    "                           \"errorLocation\" : 3, \n",
    "                           \"codeOpinion\" : 4, \n",
    "                           \"originalCode\" : 5, \n",
    "                           \"proposedNewCode\" : 5, \n",
    "                           \"platformRelated\" : 6} # FIXME this one is temporary\n",
    "\n",
    "def chooseContentDomain(ser):\n",
    "    if len(ser) == 1:\n",
    "        return ser.iloc[0].split(\" > \")[-1]\n",
    "    elif len(ser) == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    ls = ser.str.split(\" > \").str[-1].to_list()\n",
    "    c0 = ls[0]\n",
    "    p0 = contentDomainPriorities[c0]\n",
    "    for c1 in ls[1:]:\n",
    "        p1 = contentDomainPriorities[c1]\n",
    "        if p0 < p1 or c0 == c1:\n",
    "            continue\n",
    "        elif p0 > p1:\n",
    "            c0, p0 = c1, p1\n",
    "        else: # p0 == p1\n",
    "            q0 = contentDomainsByCount[c0]\n",
    "            q1 = contentDomainsByCount[c1]\n",
    "            c0, p0 = (c0, p0) if q0 < q1 else (c1, p1)\n",
    "    return c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f81ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function: given multiple communicationMechanisms, choose the highest-priority\n",
    "# and breaking ties in favor of the less-frequent code\n",
    "commMechsByCount = da2.loc[da1[\"annotation.code\"].str.startswith(\"Explanations and help > communicationMechanism\"), \n",
    "                               \"annotation.code\"].str.split(\" > \").str[-1].value_counts()\n",
    "commMechsByCount = commMechsByCount.sort_values(ascending=True).index.to_list()\n",
    "commMechsByCount = {i : k for k, i in enumerate(commMechsByCount)}\n",
    "#print(commMechsByCount)\n",
    "\n",
    "commMechPriorities = {\"explain\" : 0, \n",
    "                      \"implement\" : 0, \n",
    "                      \"guideInteractively\" : 0, \n",
    "                      \"suggest\" : 1, \n",
    "                      \"teachWithExtensions\" : 2, \n",
    "                      \"state\" : 3, \n",
    "                      \"positiveConfirmation\" : 4, \n",
    "                      \"negativeConfirmation\" : 4}\n",
    "\n",
    "def chooseCommunicationMechanism(ser):\n",
    "    if len(ser) == 1:\n",
    "        return ser.iloc[0].split(\" > \")[-1]\n",
    "    elif len(ser) == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    ls = ser.str.split(\" > \").str[-1].to_list()\n",
    "    c0 = ls[0]\n",
    "    p0 = commMechPriorities[c0]\n",
    "    for c1 in ls[1:]:\n",
    "        p1 = commMechPriorities[c1]\n",
    "        if p0 < p1 or c0 == c1:\n",
    "            continue\n",
    "        elif p0 > p1:\n",
    "            c0, p0 = c1, p1\n",
    "        else: # p0 == p1\n",
    "            q0 = commMechsByCount[c0]\n",
    "            q1 = commMechsByCount[c1]\n",
    "            c0, p0 = (c0, p0) if q0 < q1 else (c1, p1)\n",
    "    return c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fac76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is totally unnecessary, but while we're over-engineering things, we might as well go all the way\n",
    "# checkForFollowing, guiding, personal, checkIfCorrect\n",
    "# content\n",
    "\n",
    "# utility function: given multiple question types, choose the highest-priority\n",
    "# and breaking ties in favor of the less-frequent code\n",
    "questionsByCount = da2.loc[da1[\"annotation.code\"].str.startswith(\"Questions > question\"), \n",
    "                               \"annotation.code\"].str.split(\" > \").str[-1].value_counts()\n",
    "questionsByCount = questionsByCount.sort_values(ascending=True).index.to_list()\n",
    "questionsByCount = {i : k for k, i in enumerate(questionsByCount)}\n",
    "print(questionsByCount)\n",
    "\n",
    "questionPriorities = {\"checkForFollowing\" : 0, \n",
    "                      \"guiding\" : 0, \n",
    "                      \"personal\" : 0, \n",
    "                      \"checkIfCorrect\" : 0, \n",
    "                      \"content\" : 1}\n",
    "\n",
    "def chooseQuestionType(ser):\n",
    "    if len(ser) == 1:\n",
    "        return ser.iloc[0].split(\" > \")[-1]\n",
    "    elif len(ser) == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    ls = ser.str.split(\" > \").str[-1].to_list()\n",
    "    c0 = ls[0]\n",
    "    p0 = questionPriorities[c0]\n",
    "    for c1 in ls[1:]:\n",
    "        p1 = questionPriorities[c1]\n",
    "        if p0 < p1 or c0 == c1:\n",
    "            continue\n",
    "        elif p0 > p1:\n",
    "            c0, p0 = c1, p1\n",
    "        else: # p0 == p1\n",
    "            q0 = questionsByCount[c0]\n",
    "            q1 = questionsByCount[c1]\n",
    "            c0, p0 = (c0, p0) if q0 < q1 else (c1, p1)\n",
    "    return c0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb45d4",
   "metadata": {},
   "source": [
    "#### b. Extract the helping quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd649d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all quote GUIDs associated with \"Explanations & help\" via various criteria that \n",
    "# agree theoretically but probably not in practive\n",
    "\n",
    "# Helping instances and communicationMechanism annotations should correspond exactly\n",
    "help_quote_guid1 = np.sort(da1.loc[da1[\"annotation.code\"].str.startswith(\n",
    "    \"Explanations and help > communicationMechanism\"), \"quote.guid\"].unique())\n",
    "\n",
    "# Helping instances and confidenceLevel annotations should correspond exactly\n",
    "help_quote_guid2 = np.sort(da1.loc[da1[\"annotation.code\"].str.startswith(\n",
    "    \"Explanations and help > confidenceLevel\"), \"quote.guid\"].unique())\n",
    "\n",
    "# Helping instances should be a subset of contentDomain instances (Questioning\n",
    "# instances also have these)\n",
    "help_quote_guid3 = np.sort(da1.loc[da1[\"annotation.code\"].str.startswith(\n",
    "    \"General message attributes > contentDomain\"), \"quote.guid\"].unique())\n",
    "\n",
    "# [explain, suggest, guideinteractively, & teachW/extensions] Helping instances should \n",
    "# be a subset of specificity instances (Questioning instances also have these)\n",
    "help_quote_guid4 = np.sort(da1.loc[da1[\"annotation.code\"].str.startswith(\n",
    "    \"Questions > specificity\"), \"quote.guid\"].unique())\n",
    "\n",
    "len(help_quote_guid1), len(help_quote_guid2), len(help_quote_guid3), len(help_quote_guid4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41976522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate out the communication mechanisms determining whether the Helping instance has 3 vs 4 attributes\n",
    "commMechs4 = [\"Explanations and help > communicationMechanism > explain\", \n",
    "              \"Explanations and help > communicationMechanism > suggest\", \n",
    "              \"Explanations and help > communicationMechanism > guideInteractively\", \n",
    "              \"Explanations and help > communicationMechanism > teachWithExtensions\"]\n",
    "\n",
    "# derive the complement\n",
    "commMechs3 = da1[\"annotation.code\"].unique()\n",
    "commMechs3 = commMechs3.astype(\"U\")\n",
    "commMechs3 = commMechs3[np.char.startswith(commMechs3, \"Explanations and help > communicationMechanism\")]\n",
    "commMechs3 = commMechs3.astype(\"object\")\n",
    "commMechs3 = np.delete(commMechs3, np.isin(commMechs3, commMechs4))\n",
    "commMechs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08110acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corresponding quote GUIDs\n",
    "help_quote_guid5 = np.sort(da1.loc[da1[\"annotation.code\"].isin(commMechs3), \"quote.guid\"].unique())\n",
    "help_quote_guid6 = np.sort(da1.loc[da1[\"annotation.code\"].isin(commMechs4), \"quote.guid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the shared part of the filter\n",
    "# 1. annotation belongs to a quote with a confidenceLevel annotation\n",
    "#    too many messages are missing these, so we'll deal with it later instead\n",
    "#help_filter = da1[\"quote.guid\"].isin(help_quote_guid2)\n",
    "\n",
    "# 2. annotation belongs to a quote with a contentDomain annotation\n",
    "help_filter = da1[\"quote.guid\"].isin(help_quote_guid3)\n",
    "\n",
    "#print(da1.loc[help_filter, \"annotation.code\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41106d1b",
   "metadata": {},
   "source": [
    "##### (i) Helping quotes with 3 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ca8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the filter for Helping messages with 3 attributes\n",
    "\n",
    "# 3. annotation belongs to one of the three categories of attributes\n",
    "help3_filter = da1[\"annotation.code\"].str.startswith(\"Explanations and help > confidenceLevel\")\n",
    "help3_filter |= da1[\"annotation.code\"].str.startswith(\"General message attributes > contentDomain\")\n",
    "#help3_filter |= da1[\"annotation.code\"].str.startswith(\"Questions > specificity\")\n",
    "for commMech in commMechs3:\n",
    "    help3_filter |= (da1[\"annotation.code\"] == commMech)\n",
    "\n",
    "# 4. annotation belongs to a quote with a communicationMechanism annotation in the relevant category\n",
    "help3_filter &= help_filter & da1[\"quote.guid\"].isin(help_quote_guid5)\n",
    "help3_filter.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the results dataframe for this category\n",
    "da2_help3 = da2[help3_filter]\n",
    "\n",
    "da2_help3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42705e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2_help3.groupby(by=\"quote.guid\").count()[\"quote.text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa4338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = da2_help3.groupby(by=\"quote.guid\")\n",
    "tmp = tmp.aggregate({\"quote.text\" : \"count\", \n",
    "                     \"document.name\" : \"first\", \n",
    "                     \"annotation.creatingUser\" : \"first\", \n",
    "                     \"annotation.code\" : \n",
    "                     [lambda ser : ser.str.startswith(\"Explanations and help > communicationMechanism\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"Explanations and help > confidenceLevel\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"General message attributes > contentDomain\").sum()]\n",
    "                    })\n",
    "tmp.columns = [\"count\", \"document.name\", \"annotation.creatingUser\", \"commMech.count\", \"conf.count\", \"content.count\"]\n",
    "tmp[\"dist\"] = list(zip(tmp[\"commMech.count\"].to_list(), tmp[\"conf.count\"].to_list(), tmp[\"content.count\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff994933",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"dist\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a007d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show(tmp[tmp[\"count\"] != 3], cols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438355a",
   "metadata": {},
   "source": [
    "Code to check the above annotations without exactly 3 attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1c470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# display(da2_help3[da2_help3[\"quote.guid\"] == \"10B67B86-6785-471B-BD23-E62282DC1105\"])\n",
    "# pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30553869",
   "metadata": {},
   "source": [
    "Construct the combined dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9c240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note this works because I've already verified that all invalid Helping quotes\n",
    "# have too many contentDomains XOR no confidenceLevel, and no other problems\n",
    "def combinecodes_help3(codeser):\n",
    "    commMechs = codeser[codeser.str.startswith(\"Explanations and help > communicationMechanism\")]\n",
    "    confLevels = codeser[codeser.str.startswith(\"Explanations and help > confidenceLevel\")].unique()\n",
    "    confLvl = confLevels[0].split(\" > \")[-1] if len(confLevels) == 1 else \"unknown\"\n",
    "    contentDomains = codeser[codeser.str.startswith(\"General message attributes > contentDomain\")]\n",
    "    return \"Helping > ({}, {}, {})\".format(chooseCommunicationMechanism(commMechs), \n",
    "                                           confLvl, \n",
    "                                           chooseContentDomain(contentDomains))\n",
    "\n",
    "agg_dict = {col : \"first\" for col in da2_help3.columns}\n",
    "agg_dict[\"annotation.code\"] = combinecodes_help3\n",
    "\n",
    "da2_help3 = da2_help3.groupby(by=\"quote.guid\").aggregate(agg_dict).reset_index(drop=True)\n",
    "show(da2_help3, cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2_help3[\"annotation.code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d2f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare with theoretical number of possibilities\n",
    "# (n commMechs3) * (n certainties + 1) * (n contentDomains)\n",
    "(4) * (2 + 1) * (14)\n",
    "#np.sort(da1[\"annotation.code\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11bef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows describing the codes, broken down\n",
    "da2_help3[\"code.primary\"] = \"Helping\"\n",
    "\n",
    "da2_help3[\"code.communicationMechanism\"] = da2_help3[\"annotation.code\"].str.split(\" > \").str[1].str.split(\", \").str[0].str[1:]\n",
    "da2_help3[\"code.confidenceLevel\"] = da2_help3[\"annotation.code\"].str.split(\", \").str[1]\n",
    "da2_help3[\"code.contentDomain\"] = da2_help3[\"annotation.code\"].str.split(\", \").str[2].str[:-1]\n",
    "\n",
    "da2_help3[\"code.questionType\"] = \"N/A\"\n",
    "da2_help3[\"code.specificity\"] = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a0449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6d0a892",
   "metadata": {},
   "source": [
    "##### (iI) Helping quotes with 4 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79098c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the filter for Helping messages with 4 attributes\n",
    "\n",
    "# 3. annotation belongs to a quote with a communicationMechanism annotation in the relevant category\n",
    "help_filter &= da1[\"quote.guid\"].isin(help_quote_guid6)\n",
    "\n",
    "# 4. annotation belongs to a quote with a specificity instance\n",
    "#    too many messages are missing these, so we'll deal with it later instead\n",
    "#help_filter &= da1[\"quote.guid\"].isin(help_quote_guid4)\n",
    "\n",
    "# 5. annotation belongs to one of the four categories of attributes\n",
    "help4_filter = da1[\"annotation.code\"].str.startswith(\"Explanations and help > confidenceLevel\")\n",
    "help4_filter |= da1[\"annotation.code\"].str.startswith(\"General message attributes > contentDomain\")\n",
    "help4_filter |= da1[\"annotation.code\"].str.startswith(\"Questions > specificity\")\n",
    "for commMech in commMechs4:\n",
    "    help4_filter |= (da1[\"annotation.code\"] == commMech)\n",
    "\n",
    "help4_filter &= help_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the results dataframe for this category\n",
    "da2_help4 = da2[help4_filter]\n",
    "\n",
    "# keep track of the leftovers\n",
    "da2 = da2[~help3_filter & ~help4_filter]\n",
    "\n",
    "da2_help4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2_help4.groupby(by=\"quote.guid\").count()[\"quote.text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = da2_help4.groupby(by=\"quote.guid\")\n",
    "tmp = tmp.aggregate({\"quote.text\" : \"count\", \n",
    "                     \"document.name\" : \"first\", \n",
    "                     \"annotation.creatingUser\" : \"first\", \n",
    "                     \"annotation.code\" : \n",
    "                     [lambda ser : ser.str.startswith(\"Explanations and help > communicationMechanism\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"Explanations and help > confidenceLevel\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"General message attributes > contentDomain\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"Questions > specificity\").sum()]\n",
    "                    })\n",
    "tmp.columns = [\"count\", \"document.name\", \"annotation.creatingUser\", \n",
    "               \"commMech.count\", \"conf.count\", \"content.count\", \"specificity.count\"]\n",
    "\n",
    "tmp[\"dist\"] = list(zip(tmp[\"commMech.count\"].to_list(), tmp[\"conf.count\"].to_list(), \n",
    "                       tmp[\"content.count\"].to_list(), tmp[\"specificity.count\"].to_list()))\n",
    "\n",
    "#tmp[tmp[\"count\"] != 4]\n",
    "tmp[\"dist\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffef389",
   "metadata": {},
   "source": [
    "Code to check the above annotations without exactly one of each category of attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215b67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# display(da2_help4[da2_help4[\"quote.guid\"] == \"027E3617-0493-423B-9819-3CE56C88DE99\"])\n",
    "# pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e11012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def combinecodes_help4(codeser):\n",
    "    commMechs = codeser[codeser.str.startswith(\"Explanations and help > communicationMechanism\")]\n",
    "    confLevels = codeser[codeser.str.startswith(\"Explanations and help > confidenceLevel\")].unique()\n",
    "    confLvl = confLevels[0].split(\" > \")[-1] if len(confLevels) == 1 else \"unknown\"\n",
    "    contentDomains = codeser[codeser.str.startswith(\"General message attributes > contentDomain\")]\n",
    "    specificities = codeser[codeser.str.startswith(\"Questions > specificity\")].unique()\n",
    "    spec = specificities[0].split(\" > \")[-1] if len(specificities) == 1 else \"unknown\"\n",
    "    return \"Helping > ({}, {}, {}, {})\".format(chooseCommunicationMechanism(commMechs), \n",
    "                                               confLvl, \n",
    "                                               chooseContentDomain(contentDomains), \n",
    "                                               spec)\n",
    "\n",
    "agg_dict = {col : \"first\" for col in da2_help4.columns}\n",
    "agg_dict[\"annotation.code\"] = combinecodes_help4\n",
    "\n",
    "da2_help4 = da2_help4.groupby(by=\"quote.guid\").aggregate(agg_dict).reset_index(drop=True)\n",
    "show(da2_help4, cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2_help4[\"annotation.code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with theoretical number of possibilities\n",
    "# (n commMechs4) * (n certainties + 1) * (n contentDomains) * (n specificities + 1)\n",
    "(4) * (2 + 1) * (14) * (2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows describing the codes, broken down\n",
    "\n",
    "da2_help4[\"code.primary\"] = \"Helping\"\n",
    "\n",
    "da2_help4[\"code.communicationMechanism\"] = da2_help4[\"annotation.code\"].str.split(\" > \").str[1].str.split(\", \").str[0].str[1:]\n",
    "da2_help4[\"code.confidenceLevel\"] = da2_help4[\"annotation.code\"].str.split(\", \").str[1]\n",
    "da2_help4[\"code.contentDomain\"] = da2_help4[\"annotation.code\"].str.split(\", \").str[2]\n",
    "da2_help4[\"code.specificity\"] = da2_help4[\"annotation.code\"].str.split(\", \").str[3].str[:-1]\n",
    "\n",
    "da2_help4[\"code.questionType\"] = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b767ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cc92b17",
   "metadata": {},
   "source": [
    "#### c. Extract the questioning quotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all quote GUIDs associated with \"Questions\" via various criteria\n",
    "\n",
    "# Questioning instances and questions annotations should correspond exactly\n",
    "ques_quote_guid1 = np.sort(da2.loc[da2[\"annotation.code\"].str.startswith(\n",
    "    \"Questions > question\"), \"quote.guid\"].unique())\n",
    "\n",
    "# Questioning should be a subset of contentDomain annotations (Helping instances also have these)\n",
    "ques_quote_guid2 = np.sort(da2.loc[da2[\"annotation.code\"].str.startswith(\n",
    "    \"General message attributes > contentDomain\"), \"quote.guid\"].unique())\n",
    "\n",
    "# Questioning instances should be a subset of specificity annotations (Helping instances also have these)\n",
    "ques_quote_guid3 = np.sort(da2.loc[da2[\"annotation.code\"].str.startswith(\n",
    "    \"Questions > specificity\"), \"quote.guid\"].unique())\n",
    "\n",
    "len(ques_quote_guid1), len(ques_quote_guid2), len(ques_quote_guid3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the filter\n",
    "# 1. annotation belongs to a quote with a questions annotation\n",
    "ques_filter = da2[\"quote.guid\"].isin(ques_quote_guid1)\n",
    "\n",
    "# 2. annotation belongs to a quote with a contentDomain annotation\n",
    "ques_filter &= da2[\"quote.guid\"].isin(ques_quote_guid2)\n",
    "\n",
    "# 3. annotation belongs to a quote with a specificity annotation\n",
    "#    too many messages are missing these, so we'll deal with it later instead\n",
    "#ques_filter &= da2[\"quote.guid\"].isin(ques_quote_guid3)\n",
    "\n",
    "tmp = da2[\"annotation.code\"].str.startswith(\"Questions > question\")\n",
    "tmp |= da2[\"annotation.code\"].str.startswith(\"General message attributes > contentDomain\")\n",
    "tmp |= da2[\"annotation.code\"].str.startswith(\"Questions > specificity\")\n",
    "ques_filter &= tmp\n",
    "\n",
    "#print(da2.loc[ques_filter, \"annotation.code\"].value_counts().sort_index())\n",
    "ques_filter.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the results dataframe for this category\n",
    "da2_ques = da2[ques_filter]\n",
    "\n",
    "# keep track of the leftovers\n",
    "da2 = da2[~ques_filter]\n",
    "\n",
    "da2_ques.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2_ques.groupby(by=\"quote.guid\").count()[\"quote.text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = da2_ques.groupby(by=\"quote.guid\")\n",
    "tmp = tmp.aggregate({\"quote.text\" : \"count\", \n",
    "                     \"document.name\" : \"first\", \n",
    "                     \"annotation.creatingUser\" : \"first\", \n",
    "                     \"annotation.code\" : \n",
    "                     [lambda ser : ser.str.startswith(\"Questions > question\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"General message attributes > contentDomain\").sum(), \n",
    "                      lambda ser : ser.str.startswith(\"Questions > specificity\").sum()]\n",
    "                    })\n",
    "tmp.columns = [\"count\", \"document.name\", \"annotation.creatingUser\", \n",
    "               \"quesType.count\", \"content.count\", \"specificity.count\"]\n",
    "\n",
    "tmp[\"dist\"] = list(zip(tmp[\"quesType.count\"].to_list(), \n",
    "                       tmp[\"content.count\"].to_list(), \n",
    "                       tmp[\"specificity.count\"].to_list()))\n",
    "\n",
    "tmp[\"dist\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944d15c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def combinecodes_ques(codeser):\n",
    "    #if len(codeser) > 3:\n",
    "    #    contentDomains = codeser[codeser.str.startswith(\"General message attributes > contentDomain\")]\n",
    "    #    codeser = codeser[~codeser.str.startswith(\"General message attributes > contentDomain\")]\n",
    "    #    codeser[contentDomains.index[0]] = chooseContentDomain(contentDomains)\n",
    "    \n",
    "    #attr = \", \".join(codeser.sort_values().str.split(\" > \").str[-1])\n",
    "    #return \"Questioning > ({})\".format(attr)\n",
    "    \n",
    "    contentDomains = codeser[codeser.str.startswith(\"General message attributes > contentDomain\")]\n",
    "    specificities = codeser[codeser.str.startswith(\"Questions > specificity\")].unique()\n",
    "    spec = specificities[0].split(\" > \")[-1] if len(specificities) == 1 else \"unknown\"\n",
    "    questions = codeser[codeser.str.startswith(\"Questions > question\")]\n",
    "    return \"Questioning > ({}, {}, {})\".format(chooseContentDomain(contentDomains), \n",
    "                                               spec, \n",
    "                                               chooseQuestionType(questions))\n",
    "\n",
    "agg_dict = {col : \"first\" for col in da2_ques.columns}\n",
    "agg_dict[\"annotation.code\"] = combinecodes_ques\n",
    "\n",
    "da2_ques = da2_ques.groupby(by=\"quote.guid\").aggregate(agg_dict).reset_index(drop=True)\n",
    "show(da2_ques, cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows describing the codes, broken down\n",
    "\n",
    "da2_ques[\"code.primary\"] = \"Questioning\"\n",
    "\n",
    "da2_ques[\"code.contentDomain\"] = da2_ques[\"annotation.code\"].str.split(\" > \").str[1].str.split(\", \").str[0].str[1:]\n",
    "da2_ques[\"code.specificity\"] = da2_ques[\"annotation.code\"].str.split(\", \").str[1]\n",
    "da2_ques[\"code.questionType\"] = da2_ques[\"annotation.code\"].str.split(\", \").str[2].str[:-1]\n",
    "\n",
    "da2_ques[\"code.communicationMechanism\"] = \"N/A\"\n",
    "da2_ques[\"code.confidenceLevel\"] = \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bff949",
   "metadata": {},
   "source": [
    "#### d. Join everybody back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6121a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a small number of miscellaneous Helping and Questioning attributes that were lying loose\n",
    "# around the dataset are going to be dropped, but that's okay -- let's check how many\n",
    "tmp = da2[da2[\"annotation.code\"].str.startswith(\"Explanations and help\") | \n",
    "          da2[\"annotation.code\"].str.startswith(\"General message attributes\") | \n",
    "          da2[\"annotation.code\"].str.startswith(\"Questions\")]\n",
    "print(len(tmp))\n",
    "tmp[\"annotation.code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17332f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2_others = da2[~(da2[\"annotation.code\"].str.startswith(\"Explanations and help\") | \n",
    "                   da2[\"annotation.code\"].str.startswith(\"General message attributes\") | \n",
    "                   da2[\"annotation.code\"].str.startswith(\"Questions\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30223a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add rows describing the codes, broken down\n",
    "\n",
    "da2_others[\"code.primary\"] = da2_others[\"annotation.code\"].copy()\n",
    "\n",
    "da2_others[\"code.communicationMechanism\"] = \"N/A\"\n",
    "da2_others[\"code.confidenceLevel\"] = \"N/A\"\n",
    "da2_others[\"code.contentDomain\"] = \"N/A\"\n",
    "da2_others[\"code.questionType\"] = \"N/A\"\n",
    "da2_others[\"code.specificity\"] = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3afa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da2 = pd.concat([da2_help3, da2_help4, da2_ques, da2_others], axis=0)\n",
    "da2 = da2.sort_values([\"document.name\", \"quote.startPosition\", \"quote.endPosition\", \"annotation.creatingUser\"])\n",
    "da2 = da2.reset_index(drop=True)\n",
    "assert(len(da2) == len(da2_help3) + len(da2_help4) + len(da2_ques) + len(da2_others))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4f222",
   "metadata": {},
   "source": [
    "#### e. Do some postprocessing on masked code columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2[\"annotation.code.noOutcome\"] = np.where(da2[\"annotation.code\"].str.startswith(\"Helping > \") | \n",
    "                                            da2[\"annotation.code\"].str.startswith(\"Questioning > \"), \n",
    "                                            da2[\"annotation.code\"], da2[\"annotation.code.noOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32c08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da2[\"annotation.code.noRequestOutcome\"] = np.where(da2[\"annotation.code\"].str.startswith(\"Helping > \") | \n",
    "                                                   da2[\"annotation.code\"].str.startswith(\"Questioning > \"), \n",
    "                                                   da2[\"annotation.code\"], da2[\"annotation.code.noRequestOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4be9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(da2, cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2[\"code.communicationMechanism\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9df7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2[\"code.confidenceLevel\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27df88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2[\"code.contentDomain\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2[\"code.specificity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f80ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2[\"code.questionType\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074def1",
   "metadata": {},
   "source": [
    "#### f. Generate the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annser = pd.Series(np.sort(da1[\"annotation.code\"].unique()))\n",
    "short_commMechs3 = pd.Series(commMechs3).str.split(\" > \").str[-1].to_list()\n",
    "short_commMechs4 = pd.Series(commMechs4).str.split(\" > \").str[-1].to_list()\n",
    "confidenceLevels = annser[annser.str.startswith(\"Explanations and help > confidenceLevel\")].str.split(\" > \").str[-1].to_list() + [\"unknown\"]\n",
    "contentDomains = annser[annser.str.startswith(\"General message attributes > contentDomain\")].str.split(\" > \").str[-1].to_list()\n",
    "specificities = annser[annser.str.startswith(\"Questions > specificity\")].str.split(\" > \").str[-1].to_list() + [\"unknown\"]\n",
    "questionTypes = annser[annser.str.startswith(\"Questions > question\")].str.split(\" > \").str[-1].to_list()\n",
    "others = annser[~annser.str.startswith(\"Explanations and help\") & \n",
    "                ~annser.str.startswith(\"General message attributes > contentDomain\") & \n",
    "                ~annser.str.startswith(\"Questions\")].to_list()\n",
    "\n",
    "help3_codes = np.array(np.meshgrid(short_commMechs3, confidenceLevels, contentDomains), dtype=\"object\")\n",
    "help3_codes = help3_codes.T.reshape([-1, 3])\n",
    "help3_codes = list(map(lambda ls : \"Helping > ({})\".format(\", \".join(ls)), help3_codes))\n",
    "\n",
    "help4_codes = np.array(np.meshgrid(short_commMechs4, confidenceLevels, contentDomains, specificities), dtype=\"object\")\n",
    "help4_codes = help4_codes.T.reshape([-1, 4])\n",
    "help4_codes = list(map(lambda ls : \"Helping > ({})\".format(\", \".join(ls)), help4_codes))\n",
    "\n",
    "question_codes = np.array(np.meshgrid(contentDomains, specificities, questionTypes), dtype=\"object\")\n",
    "question_codes = question_codes.T.reshape([-1, 3])\n",
    "question_codes = list(map(lambda ls : \"Questioning > ({})\".format(\", \".join(ls)), question_codes))\n",
    "\n",
    "da2_codes = help3_codes + help4_codes + question_codes + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbec252",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(da2_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3203585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of codes if we didn't have \"unknown\" as an option\n",
    "(4 * 2 * 14) + (4 * 2 * 14 * 2) + (5 * 14 * 2) + len(others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d38954",
   "metadata": {},
   "source": [
    "#### g. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    output_longform(da2, outputdir, codebook2_annotations_output)\n",
    "    #da2.to_csv(os.path.join(outputdir, codebook2_annotations_output), index=False)\n",
    "    pd.Series(da2_codes).to_csv(os.path.join(outputdir, codebook2_codes_output), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51b8ba",
   "metadata": {},
   "source": [
    "### 3. Construct the third codebook and dataframe (`da3`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036190f5",
   "metadata": {},
   "source": [
    "#### a. Regenerate the codes in the long-form dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc28ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da3 = da2.copy()\n",
    "da3 = da3.drop([\"code.confidenceLevel\", \"code.specificity\"], axis=\"columns\")\n",
    "da3[\"annotation.code\"] = np.where(da3[\"code.primary\"] == \"Helping\", \n",
    "                                  np.frompyfunc(\"Helping > ({}, {})\".format, 2, 1)(\n",
    "                                      da3[\"code.communicationMechanism\"], da3[\"code.contentDomain\"]), \n",
    "                                  da3[\"annotation.code\"])\n",
    "da3[\"annotation.code\"] = np.where(da3[\"code.primary\"] == \"Questioning\", \n",
    "                                  np.frompyfunc(\"Questioning > ({}, {})\".format, 2, 1)(\n",
    "                                      da3[\"code.questionType\"], da3[\"code.contentDomain\"]), \n",
    "                                  da3[\"annotation.code\"])\n",
    "show(da3, cols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524b444",
   "metadata": {},
   "source": [
    "#### b. Do some postprocessing on masked code columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc20245",
   "metadata": {},
   "outputs": [],
   "source": [
    "da3[\"annotation.code.noOutcome\"] = np.where(da3[\"annotation.code\"].str.startswith(\"Helping > \") | \n",
    "                                            da3[\"annotation.code\"].str.startswith(\"Questioning > \"), \n",
    "                                            da3[\"annotation.code\"], da3[\"annotation.code.noOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc15554",
   "metadata": {},
   "outputs": [],
   "source": [
    "da3[\"annotation.code.noRequestOutcome\"] = np.where(da3[\"annotation.code\"].str.startswith(\"Helping > \") | \n",
    "                                                   da3[\"annotation.code\"].str.startswith(\"Questioning > \"), \n",
    "                                                   da3[\"annotation.code\"], da3[\"annotation.code.noRequestOutcome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3c0e8",
   "metadata": {},
   "source": [
    "#### c. Generate the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many codes were used\n",
    "da3[\"annotation.code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874edca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the codebook size for comparison\n",
    "(4 * 14) + (4 * 14) + (5 * 14) + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "commMechs = annser[annser.str.startswith(\"Explanations and help > communicationMechanism\")].str.split(\" > \").str[-1].to_list()\n",
    "\n",
    "help_codes = np.array(np.meshgrid(commMechs, contentDomains), dtype=\"object\")\n",
    "help_codes = help_codes.T.reshape([-1, 2])\n",
    "help_codes = list(map(lambda ls : \"Helping > ({})\".format(\", \".join(ls)), help_codes))\n",
    "\n",
    "ques_codes = np.array(np.meshgrid(questionTypes, contentDomains), dtype=\"object\")\n",
    "ques_codes = ques_codes.T.reshape([-1, 2])\n",
    "ques_codes = list(map(lambda ls : \"Questioning > ({})\".format(\", \".join(ls)), ques_codes))\n",
    "\n",
    "da3_codes = help_codes + ques_codes + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(da3_codes) # matches the hardcoded calculation, yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b393a",
   "metadata": {},
   "source": [
    "#### d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8265df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    output_longform(da3, outputdir, codebook3_annotations_output)\n",
    "    pd.Series(da3_codes).to_csv(os.path.join(outputdir, codebook3_codes_output), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc368ea4",
   "metadata": {},
   "source": [
    "### 4. Construct the fourth codebook and dataframe (`da4`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e748e4",
   "metadata": {},
   "source": [
    "#### a. Do the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdabbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "da4 = da3.copy()\n",
    "\n",
    "# Merge guiding codes\n",
    "fil = da4[\"code.questionType\"] == \"guiding\"\n",
    "da4.loc[fil, \"code.primary\"] = \"Helping\"\n",
    "da4.loc[fil, \"code.questionType\"] = \"N/A\"\n",
    "da4.loc[fil, \"code.communicationMechanism\"] = \"guideInteractively\"\n",
    "da4[\"annotation.code\"] = np.where(fil, \n",
    "                                  np.frompyfunc(\"Helping > ({}, {})\".format, 2, 1)(\n",
    "                                      da4[\"code.communicationMechanism\"], da4[\"code.contentDomain\"]), \n",
    "                                  da4[\"annotation.code\"])\n",
    "\n",
    "# Merge confirmation codes\n",
    "fil = da4[\"code.communicationMechanism\"].str.endswith(\"confirmation\")\n",
    "da4.loc[fil, \"code.communicationMechanism\"] = \"confirmation\"\n",
    "da4[\"annotation.code\"] = np.where(fil, \n",
    "                                  np.frompyfunc(\"Helping > ({}, {})\".format, 2, 1)(\n",
    "                                      da4[\"code.communicationMechanism\"], da4[\"code.contentDomain\"]), \n",
    "                                  da4[\"annotation.code\"])\n",
    "\n",
    "# Merge contentDomain codes\n",
    "grp = {\"proposedNewCode\"     : \"sourceCode\", \n",
    "       \"originalCode\"        : \"sourceCode\", \n",
    "       \"codeOpinion\"         : \"sourceCode\", \n",
    "       \"bug\"                 : \"codeError\", \n",
    "       \"errorLocation\"       : \"codeError\", \n",
    "       \"errorMsg\"            : \"codeError\", \n",
    "       \"codingConcept\"       : \"higherLevelInstruction\", \n",
    "       \"developmentStrategy\" : \"higherLevelInstruction\", \n",
    "       \"learningResources\"   : \"higherLevelInstruction\", \n",
    "       \"codingExperience\"    : \"rapportBuilding\", \n",
    "       \"personalInfo\"        : \"rapportBuilding\"}\n",
    "fil = da4[\"code.contentDomain\"] != \"N/A\"\n",
    "da4[\"code.contentDomain\"] = np.frompyfunc(lambda c : grp[c] if c in grp.keys() else c, \n",
    "                                          1, 1)(da4[\"code.contentDomain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70173d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da4[\"annotation.code\"] = np.where(da4[\"code.primary\"] == \"Helping\", \n",
    "                                  np.frompyfunc(\"Helping > ({}, {})\".format, 2, 1)(\n",
    "                                      da4[\"code.communicationMechanism\"], da4[\"code.contentDomain\"]), \n",
    "                                  da4[\"annotation.code\"])\n",
    "da4[\"annotation.code\"] = np.where(da4[\"code.primary\"] == \"Questioning\", \n",
    "                                  np.frompyfunc(\"Questioning > ({}, {})\".format, 2, 1)(\n",
    "                                      da4[\"code.questionType\"], da4[\"code.contentDomain\"]), \n",
    "                                  da4[\"annotation.code\"])\n",
    "show(da4, cols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect 529 guideInteractively instances\n",
    "da4[\"code.communicationMechanism\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be510b",
   "metadata": {},
   "source": [
    "#### b. Do some postprocessing on masked code columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "da4[\"annotation.code.noOutcome\"] = np.where(da4[\"annotation.code\"].str.startswith(\"Helping > \") | \n",
    "                                            da4[\"annotation.code\"].str.startswith(\"Questioning > \"), \n",
    "                                            da4[\"annotation.code\"], da4[\"annotation.code.noOutcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fae0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "da4[\"annotation.code.noRequestOutcome\"] = np.where(da4[\"annotation.code\"].str.startswith(\"Helping > \") | \n",
    "                                                   da4[\"annotation.code\"].str.startswith(\"Questioning > \"), \n",
    "                                                   da4[\"annotation.code\"], da4[\"annotation.code.noRequestOutcome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00c333",
   "metadata": {},
   "source": [
    "#### c. Generate the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many codes were used\n",
    "da4[\"annotation.code\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the codebook size for comparison\n",
    "(7 * 7) + (4 * 7) + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionTypes.remove(\"guiding\")\n",
    "\n",
    "commMechs.remove(\"positiveConfirmation\")\n",
    "commMechs.remove(\"negativeConfirmation\")\n",
    "commMechs.append(\"confirmation\")\n",
    "\n",
    "contentDomains = np.concatenate([[c for c in contentDomains if not c in grp.keys()], np.unique(list(grp.values()))])\n",
    "\n",
    "help_codes = np.array(np.meshgrid(commMechs, contentDomains), dtype=\"object\")\n",
    "help_codes = help_codes.T.reshape([-1, 2])\n",
    "help_codes = list(map(lambda ls : \"Helping > ({})\".format(\", \".join(ls)), help_codes))\n",
    "\n",
    "ques_codes = np.array(np.meshgrid(questionTypes, contentDomains), dtype=\"object\")\n",
    "ques_codes = ques_codes.T.reshape([-1, 2])\n",
    "ques_codes = list(map(lambda ls : \"Questioning > ({})\".format(\", \".join(ls)), ques_codes))\n",
    "\n",
    "da4_codes = help_codes + ques_codes + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bf774",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(da4_codes) # matches the hardcoded calculation, yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca314f",
   "metadata": {},
   "source": [
    "#### d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584297f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    output_longform(da4, outputdir, codebook4_annotations_output)\n",
    "    pd.Series(da4_codes).to_csv(os.path.join(outputdir, codebook4_codes_output), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efef24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0233a7f",
   "metadata": {},
   "source": [
    "## B. 1-gram frequency dataframes\n",
    "\n",
    "REQUIRES: Section A has been run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a2c98",
   "metadata": {},
   "source": [
    "### 1. Construct code-outcome counts dataframe `countsda1`\n",
    "\n",
    "**Pooled `speaker`, `conversation`, `annotator`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd28d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each version of the codebook\n",
    "for k, (da, annls) in enumerate([(da1, da1_codes), (da2, da2_codes), (da3, da3_codes), (da4, da4_codes)]):\n",
    "    # build the dataframe\n",
    "    countsda1 = da[[\"annotation.code.noOutcome\", \n",
    "                    \"voted.interaction.outcome\"]].value_counts()                         # compute the frequencies\n",
    "\n",
    "    for pr in np.array(np.meshgrid(annls, [\"F\", \"S\"])).T.reshape(-1,2):                  # fill in empty rows\n",
    "        idx = tuple(pr)\n",
    "        if not idx in countsda1.index:\n",
    "            countsda1[idx] = 0\n",
    "\n",
    "    countsda1 = countsda1.sort_index().to_frame().reset_index()                          # fix the formatting\n",
    "\n",
    "    countsda1 = countsda1.rename(columns={\"annotation.code.noOutcome\" : \"code\",          # fix the naming\n",
    "                                          \"voted.interaction.outcome\" : \"outcome\", \n",
    "                                          0 : \"count\"})\n",
    "    \n",
    "    # preview for debugging\n",
    "    show(countsda1, rows=5, cols=None, width=None)\n",
    "    \n",
    "    # output it\n",
    "    if output:\n",
    "        countsda1.to_csv(os.path.join(outputdir, \"codebook{}_{}\".format(k+1, code_counts_output)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a867985",
   "metadata": {},
   "source": [
    "### 2. Construct conversation-annotator-code-speaker-outcome counts dataframe `countsda2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fadc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        'quote.text', 'annotation.code', 'annotation.creatingUser',\n",
    "#        'annotation.creationDateTime', 'quote.startPosition',\n",
    "#        'quote.endPosition', 'quote.creatingUser', 'quote.creationDateTime',\n",
    "#        'quote.modifyingUser', 'quote.modifiedDateTime', 'document.name',\n",
    "#        'document.creatingUser', 'document.creationDateTime',\n",
    "#        'document.modifyingUser', 'document.modifiedDateTime',\n",
    "#        'document.plainTextPath', 'document.richTextPath', 'annotation.guid',\n",
    "#        'annotation.codeRef.guid', 'quote.guid', 'document.guid',\n",
    "#        'quote.paragraphStartPosition', 'quote.paragraphEndPosition',\n",
    "#        'quote.paragraphText', 'quote.speaker', 'quote.speakerIsLearner',\n",
    "#        'annotation.original_code', 'interaction.number', 'interaction.len',\n",
    "#        'interaction.requests', 'interaction.outcome', 'interaction.strict',\n",
    "#        'interaction.strict_len', 'annotation.code.noOutcome',\n",
    "#        'annotation.code.noRequestOutcome', 'voted.interaction.requests', 'voted.interaction.outcome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864eb752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def construct_conv_ann_speaker_outcome_longform_counts(k, da, annls):\n",
    "    # compute the frequencies\n",
    "    countsda2 = da[[\"document.name\", \"interaction.number\", \"annotation.creatingUser\", # group by interaction-annotator\n",
    "                    \"annotation.code.noOutcome\", \"quote.speakerIsLearner\",            # group by annotation and speaker\n",
    "                    \"voted.interaction.requests\", \"interaction.requests\",             # keep these around\n",
    "                    \"voted.interaction.outcome\", \"interaction.outcome\"]].value_counts()\n",
    "    countsda2 = countsda2.sort_index().to_frame()                                     # fix the formatting\n",
    "    countsda2 = countsda2.rename(columns={0 : \"count\"})                               # fix the naming\n",
    "\n",
    "    # debugging\n",
    "    #show(countsda2.head(2))\n",
    "\n",
    "    # pivot to fill in empty values (and compute conversation lengths)\n",
    "    countsda2 = countsda2.unstack(level=[\"annotation.code.noOutcome\", \"quote.speakerIsLearner\"], \n",
    "                                  fill_value=0).copy()\n",
    "    \n",
    "    # add missing code-speakers\n",
    "    cols_to_add = []\n",
    "    for pr in np.array(np.meshgrid(annls, [False, True])).T.reshape(-1,2):                  # fill in empty rows\n",
    "        idx = (\"count\", pr[0], pr[1] == \"True\") # sorry about this one\n",
    "        if not idx in countsda2.columns:\n",
    "            cols_to_add.append(idx)\n",
    "    #print(\"Included code-speakers:\", countsda2.shape[1])\n",
    "    #print(\"Missing code-speakers:\", len(cols_to_add))\n",
    "    countsda2 = pd.concat([countsda2, \n",
    "                           pd.DataFrame(0, index=countsda2.index, columns=cols_to_add)], \n",
    "                          axis = 1) #countsda2[cols_to_add] = 0\n",
    "    \n",
    "    # debugging\n",
    "    #show(countsda2, cols=None)\n",
    "    #break\n",
    "\n",
    "    # compute the conversation lengths according to each annotator (for codebook 1 this should\n",
    "    # agree with the version in the dataframe)\n",
    "    tmp = countsda2[\"count\"].sum(axis=1) #.astype(np.int64)\n",
    "    assert(tmp.min() >= 2)\n",
    "\n",
    "    # we have to make a lot of copies of this column because it doesn't broadcast automatically when we restack\n",
    "    for col in countsda2.columns:\n",
    "        countsda2[(\"interaction.length\", col[1], False)] = tmp\n",
    "        countsda2[(\"interaction.length\", col[1], True)] = tmp\n",
    "\n",
    "    # make the whole thing vertical again (apparently this fills in more empty values)\n",
    "    countsda2 = countsda2.stack(level=[\"annotation.code.noOutcome\", \"quote.speakerIsLearner\"], \n",
    "                                dropna=False)\n",
    "    countsda2 = countsda2.fillna(0).astype(np.int64)\n",
    "\n",
    "    # debugging\n",
    "    #show(countsda2, cols=None)\n",
    "\n",
    "    # clean up some formatting things\n",
    "    countsda2 = countsda2.reset_index() # level=\"interaction.outcome\"\n",
    "\n",
    "    countsda2 = countsda2.rename(columns={\"document.name\" : \"document\",  \n",
    "                                          \"interaction.number\" : \"conversation_number\",\n",
    "                                          \"annotation.creatingUser\" : \"annotator\", \n",
    "                                          \"voted.interaction.requests\" : \"request\", \n",
    "                                          \"voted.interaction.outcome\" : \"outcome\", \n",
    "                                          \"annotation.code.noOutcome\" : \"code\", \n",
    "                                          \"quote.speakerIsLearner\" : \"speakerIsLearner\", \n",
    "                                          \"interaction.requests\" : \"nominal_request\", \n",
    "                                          \"interaction.outcome\" : \"nominal_outcome\", \n",
    "                                          \"annotation.code.noRequestOutcome\" : \"code_noRequest\", \n",
    "                                          \"interaction.length\" : \"conversation_length\"})\n",
    "    # debugging\n",
    "    #print(countsda2.shape)\n",
    "    #show(countsda2.iloc[0:5])\n",
    "    #show(countsda2.iloc[94:100])\n",
    "\n",
    "    # derived columns\n",
    "    assert(len(countsda2[countsda2[\"conversation_length\"] <= 1]) == 0)               # data validity check\n",
    "    countsda2[\"ln_conversation_length\"] = np.log(countsda2[\"conversation_length\"])   # derived column (offset)\n",
    "    assert((countsda2.isnull().sum() == 0).all())                                    # data validity check FIXME\n",
    "\n",
    "    np.power(countsda2[\"count\"], 1/2).hist(figsize=(11, 4), bins=40)                 # visualize the output\n",
    "    plt.title(\"Distribution of conversation-annotator-code-speaker frequencies\")\n",
    "    plt.xlabel(\"Square root count\")\n",
    "    plt.ylabel(\"Number of conv-ann-code-speakers\")\n",
    "    plt.show()\n",
    "\n",
    "    countsda2[\"conversation_sharedID\"] = list(zip(countsda2[\"document\"],             # derived column (grouping variable)\n",
    "                                                  countsda2[\"conversation_number\"]))\n",
    "    countsda2[\"conversation_uniqueID\"] = list(zip(countsda2[\"document\"],             # derived column (grouping variable)\n",
    "                                                  countsda2[\"annotator\"], \n",
    "                                                  countsda2[\"conversation_number\"]))\n",
    "    # debugging\n",
    "    show(countsda2, rows=4, cols=None)\n",
    "\n",
    "    # output\n",
    "    if output:\n",
    "        countsda2.to_csv(os.path.join(outputdir, \n",
    "                                      \"codebook{}_{}\".format(k+1, conversation_1gram_counts_output)), \n",
    "                         index=False, \n",
    "                         compression=\"gzip\")\n",
    "    \n",
    "    # need this for the next thing\n",
    "    return countsda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each version of the codebook\n",
    "countsda2ls = []\n",
    "for k, (da, annls) in enumerate([(da1, da1_codes), (da2, da2_codes), (da3, da3_codes), (da4, da4_codes)]): \n",
    "    # preprocessing: annls includes outcomes so let's fix that\n",
    "    annls = annls.copy()\n",
    "    annls.remove(\"Big picture of an interaction > resolveRequest > failure\") \n",
    "    annls.remove(\"Big picture of an interaction > resolveRequest > success\")\n",
    "    annls.append(\"Big picture of an interaction > resolveRequest\")\n",
    "    \n",
    "    # now do the thing\n",
    "    countsda2 = construct_conv_ann_speaker_outcome_longform_counts(k, da, annls)\n",
    "    countsda2ls.append(countsda2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24844962",
   "metadata": {},
   "source": [
    "### 3. Construct conversation-annotator x code-speaker counts matrix `countsmtx1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904728e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, countsda2 in enumerate(countsda2ls):\n",
    "    # make it fat\n",
    "    countsmtx1 = countsda2.pivot(index=[\"document\", \"conversation_number\", \n",
    "                                        \"annotator\", \n",
    "                                        \"request\", \"outcome\", \n",
    "                                        \"nominal_request\", \"nominal_outcome\", \n",
    "                                        \"conversation_length\"], \n",
    "                                 columns=[\"code\", \"speakerIsLearner\"], values=\"count\")\n",
    "    # debugging\n",
    "    #show(countsmtx1.head(4))\n",
    "\n",
    "    # column name formatting (booleans to strings)\n",
    "    countsmtx1.columns = pd.MultiIndex.from_product(\n",
    "        iterables = [countsmtx1.columns.levels[0], \n",
    "                     pd.Index([\"Helper\", \"Learner\"], dtype=\"object\", name=\"speaker\")])\n",
    "    # debugging\n",
    "    #show(countsmtx1.head(4))\n",
    "\n",
    "    # column name formatting (MultiIndex to tuples)\n",
    "    countsmtx1.columns = countsmtx1.columns.to_flat_index()\n",
    "    # debugging\n",
    "    show(countsmtx1.head(4))\n",
    "\n",
    "    # row name formatting (MultiIndex to columns)\n",
    "    countsmtx1 = countsmtx1.reset_index()\n",
    "\n",
    "    # output\n",
    "    if output:\n",
    "        countsmtx1.to_csv(os.path.join(outputdir, \"codebook{}_{}\".format(k+1, countsmtx_output)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097acb31",
   "metadata": {},
   "source": [
    "## C. 2-gram frequency dataframes\n",
    "\n",
    "REQUIRES: Section A has been run\n",
    "\n",
    "NOTE: this isn't updated for new codebooks yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c99a0b",
   "metadata": {},
   "source": [
    "### 1. Construct conversation-annotator-2gram-speakers-outcome counts dataframe `countsda3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "# da        : a long-form dataframe, for a single conversation-annotator, where each item is an annotation, in sorted order\n",
    "# codecol   : specify which column to read codes from \n",
    "#             (annotation.code, annotation.code.noOutcome, or annotation.code.noOutcomeRequest)\n",
    "# annls     : alphabetical list of all codes (for indexing)\n",
    "# speakerls : [True, False] = [\"learner\", \"helper\"]\n",
    "#\n",
    "# OUTPUT\n",
    "# counts    : a counts dataframe indexed by (code 1, speaker 1, code 2, speaker 2)\n",
    "def count2grams(da, codecol, annls, speakerls=[True, False]):\n",
    "    # First, initialize a counts matrix of all zeros\n",
    "    idx = pd.MultiIndex.from_product(iterables=[annls, speakerls, annls, speakerls], \n",
    "                                     names=[\"code1\", \"speakerIsLearner1\", \"code2\", \"speakerIsLearner2\"])\n",
    "    counts = pd.DataFrame(data=0, index=idx, columns=[\"count\"], dtype=np.int64)\n",
    "    \n",
    "    # Second, iterate through the conversation, adding to the counts dataframe\n",
    "    i1, j1 = 0, 0 # start and stop indices of all colocated annotations to act as code 1\n",
    "    \n",
    "    # Find the first code 1 location range in the conversation\n",
    "    while j1 < len(da) and da.iloc[j1][\"quote.startPosition\"] == da.iloc[i1][\"quote.startPosition\"]:\n",
    "        j1 += 1\n",
    "    \n",
    "    i2, j2 = j1+1, j1+1 # start and stop indices of all colocated annotations to act as code 2\n",
    "    \n",
    "    while j1 < len(da): # for each biclique\n",
    "        # iterate over the code 2 location range\n",
    "        while j2 < len(da) and da.iloc[j2][\"quote.startPosition\"] == da.iloc[i2][\"quote.startPosition\"]:\n",
    "            # write this biclique into the counts matrix\n",
    "            for k1 in range(i1, j1):\n",
    "                counts.loc[(da.iloc[k1][codecol], da.iloc[k1][\"quote.speakerIsLearner\"], \n",
    "                           da.iloc[j2][codecol], da.iloc[j2][\"quote.speakerIsLearner\"]), \n",
    "                           \"count\"] += 1\n",
    "            j2 += 1\n",
    "        \n",
    "        # move to the next code 1 location range\n",
    "        i1, j1 = i2, j2\n",
    "        i2, j2 = j1+1, j1+1\n",
    "    \n",
    "    # store the conversation length\n",
    "    counts[\"conversation_length\"] = len(da)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf376416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count up the 2-grams\n",
    "countsda3 = da1.groupby(by=[\"document.name\", \"annotation.creatingUser\", \"interaction.number\", # group by conversation\n",
    "                            \"interaction.outcome\"])                                           # keep the outcome around\n",
    "\n",
    "countsda3 = countsda3.apply(count2grams,                                         # the function applied to each group\n",
    "                            \"annotation.code.noOutcome\",                         # second argument `codecol`\n",
    "                            np.sort(da1[\"annotation.code.noOutcome\"].unique()))  # third argument `annls`\n",
    "\n",
    "countsda3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbdc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix up the formatting\n",
    "countsda3 = countsda3.reset_index()\n",
    "\n",
    "countsda3 = countsda3.rename(columns={\"document.name\" : \"document\", \n",
    "                                      \"annotation.creatingUser\" : \"annotator\", \n",
    "                                      \"interaction.number\" : \"conversation_number\", \n",
    "                                      \"interaction.outcome\" : \"outcome\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful extras\n",
    "countsda3[\"ln_conversation_length\"] = np.log(countsda3[\"conversation_length\"])\n",
    "\n",
    "countsda3[\"conversation_sharedID\"] = list(zip(countsda3[\"document\"], \n",
    "                                              countsda3[\"conversation_number\"]))\n",
    "countsda3[\"conversation_uniqueID\"] = list(zip(countsda3[\"document\"], \n",
    "                                              countsda3[\"annotator\"], \n",
    "                                              countsda3[\"conversation_number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44069300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "display(countsda3)\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check the length of the dataframe (it's not perfect because some \n",
    "# annotators skipped some documents, but it's believable)\n",
    "print(countsda3.shape)\n",
    "print(\"\", 133*3*(48*2)**2) # num convos * num annotators * (num codes * num speakers)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    countsda3.head(20).to_csv(os.path.join(outputdir, small_conversation_2gram_counts_output), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output:\n",
    "    countsda3.to_csv(os.path.join(outputdir, conversation_2gram_counts_output), index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 3.33M rows, of which 3.30M are zeros\n",
    "countsda3[\"count\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81bf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c365c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
